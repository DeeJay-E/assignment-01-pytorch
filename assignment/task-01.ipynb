{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01: Introduction to Pytorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of this task is to get us thinking not just about training models, but about our *training pipelines*.\n",
    "\n",
    "A neural network is a function, $f$, that accepts in data inputs, $\\boldsymbol{X}$, and weights, $\\boldsymbol{\\Theta}$ that produces labels $\\boldsymbol{\\hat{y}}$,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = f(\\Theta; \\boldsymbol{X}).\n",
    "$$\n",
    "\n",
    "Meanwhile, a neural network training process, is itself a function, $g$, which accepts as input a dataset $x$, and for supervised algorithms a set of targets $y$, along with a set of parameters $\\boldsymbol{\\Omega}$ which define how the process is performed, and produces as output the weights of a neural network, $\\boldsymbol{\\Theta}$,\n",
    "\n",
    "$$\n",
    "\\Theta = g(\\boldsymbol{\\Omega}; \\boldsymbol{X}, \\boldsymbol{y}).\n",
    "$$\n",
    "\n",
    "It is helpful to think of the training function, $g$, as a pipeline, composed of several training steps, which can include preprocessing, post processing, etc.\n",
    "\n",
    "$$\n",
    "g = g_N \\circ\\ \\cdots\\ \\circ g_1.\n",
    "$$\n",
    "\n",
    "For example, $g_1$ might be a preprocessing step, then $g_2$ might be a training step, and $g_3$ might be a pruning step in a basic pipeline where data $(\\boldsymbol{X}, \\boldsymbol{y})$ goes in and weights $\\boldsymbol{\\Theta}$ come out.\n",
    "\n",
    "We will learn to think of the training process this way by modifying some example code for a basic MNIST classification task. We begin with some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01 - Part 1\n",
    "\n",
    "Your first task is to:\n",
    "\n",
    "* Add layer definitions to the following neural network class\n",
    "* Define the forward pass\n",
    "\n",
    "You can find starting architectures online. It is important to know there is no known theory to identify a best architecture *before* starting the problem. Trial and error (by iterative training and testing) is the only way to prove or disprove the utility of an architecture.\n",
    "\n",
    "That said, recall some intuition about the way linear and nonlinear transforms work. We know we need a chain of both to have any hope of solving this problem. We also know that we need some depth, and cannot solve this problem by width alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        raise NotImplementedError(\"Not implemented!\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Not implemented!\")\n",
    "        # Uncomment the return once implemented\n",
    "        # return output\"\"\"\n",
    "\n",
    "\n",
    "def run_training_epoch(\n",
    "    training_params, model, device, train_loader, optimizer, epoch\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % training_params.log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "            if training_params.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def predict(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code: Training Pipeline\n",
    "\n",
    "For this assignment, the training pipeline is defined for you. Notice the similarities to the mathematical description of a trainer we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 1000\n",
    "    epochs: int = 14\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.7\n",
    "    normalizer_mean = 0.1307\n",
    "    normalizer_std = 0.3081\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "\n",
    "\n",
    "def configure_training_device(training_params):\n",
    "    use_cuda = not training_params.no_cuda and torch.cuda.is_available()\n",
    "    use_mps = not training_params.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "    torch.manual_seed(training_params.seed)\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    train_kwargs = {\"batch_size\": training_params.batch_size}\n",
    "    test_kwargs = {\"batch_size\": training_params.test_batch_size}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "    return device, train_kwargs, test_kwargs\n",
    "\n",
    "\n",
    "def build_preprocessing_transform(training_params):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                (training_params.normalizer_mean,),\n",
    "                (training_params.normalizer_std,),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "def build_data_loaders(train_kwargs, test_kwargs, transform):\n",
    "    dataset1 = datasets.MNIST(\n",
    "        \"../data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(training_params, device, train_loader, test_loader):\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=training_params.lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=training_params.gamma)\n",
    "\n",
    "    for epoch in range(1, training_params.epochs + 1):\n",
    "        run_training_epoch(\n",
    "            training_params, model, device, train_loader, optimizer, epoch\n",
    "        )\n",
    "        predict(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        if training_params.save_model:\n",
    "            torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Execute a Training Pipeline\n",
    "\n",
    "With our training steps defined in modular fashion, we can easily define and execute a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training_pipeline():\n",
    "    training_params = TrainingParameters(epochs=1, dry_run=True)\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(\n",
    "        training_params\n",
    "    )\n",
    "    transform = build_preprocessing_transform(training_params)\n",
    "    train_loader, test_loader = build_data_loaders(\n",
    "        train_kwargs, test_kwargs, transform\n",
    "    )\n",
    "    train(training_params, device, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 - Part 2: Explore Width\n",
    "\n",
    "Using the example above, define a network with a single hidden layer.\n",
    "\n",
    "Modify the trainer to store the train and test errors in a numpy vector.\n",
    "\n",
    "Create a for loop over to iterate through a few different amounts of hidden neurons and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "(It should be hard to get good convergence here - this is part of the exercise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Linear(in_features=28, out_features=1000, bias=True)\n",
      "  (output): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(28,1000)\n",
    "        self.output = nn.Linear(1000,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "     x = F.relu(self.layer1(x))\n",
    "     x = self.output(x)\n",
    "     x = F.softmax(x.reshape(x.shape[0],-1), dim = 1)\n",
    "     return x\n",
    "    \n",
    "my_nn = Net()\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.003376\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: -0.003886\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: -0.004629\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: -0.005441\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: -0.006012\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: -0.006427\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: -0.006836\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: -0.007942\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: -0.008254\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: -0.008577\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -0.009412\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: -0.009045\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: -0.009033\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: -0.010253\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: -0.010514\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: -0.011781\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: -0.010880\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: -0.011731\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: -0.011542\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: -0.011935\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -0.010992\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: -0.011689\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: -0.009907\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: -0.012352\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: -0.013132\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: -0.010763\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: -0.018689\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: -0.008592\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: -0.014808\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: -0.006283\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -0.006498\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: -0.014915\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: -0.004252\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: -0.013297\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: -0.013208\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: -0.013470\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: -0.007707\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: -0.007684\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: -0.020926\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: -0.011564\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -0.007836\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: -0.011539\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: -0.015439\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: -0.017184\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: -0.015474\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: -0.022904\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: -0.011691\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: -0.017311\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: -0.011588\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: -0.019234\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -0.015515\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: -0.005944\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: -0.015428\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: -0.015462\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: -0.015475\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: -0.009673\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: -0.011612\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: -0.015514\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: -0.011702\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: -0.009750\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -0.021104\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: -0.021255\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: -0.015500\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: -0.015481\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: -0.019373\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: -0.017409\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: -0.015518\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: -0.015541\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: -0.015531\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: -0.011659\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -0.009741\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: -0.010795\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: -0.017305\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: -0.009763\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: -0.017421\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: -0.013622\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: -0.011697\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: -0.015547\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: -0.013335\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: -0.017513\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -0.009770\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: -0.015558\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: -0.015556\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: -0.021361\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: -0.017438\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: -0.013592\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: -0.009752\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: -0.013632\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: -0.019301\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: -0.017444\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -0.015562\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: -0.009762\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: -0.017522\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: -0.013614\n",
      "\n",
      "Test set: Average loss: -0.0141, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute_training_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 01 - Part 3: Explore Depth\n",
    "\n",
    "Now using the example above, define several networks with increasing numbers of hidden layers (either convolutional or fully connected).\n",
    "\n",
    "As above, create a for loop over to iterate through a few different depths and train until convergence (when the error stops decreasing) each time.\n",
    "\n",
    "Save the minimum error achieved and plot it with respect to the number of hidden nodes.\n",
    "\n",
    "This example should converge much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
